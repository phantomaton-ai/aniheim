# Aniheim Rendering Engine - Design Document 👻🎬

## 1. Overview

This document outlines the design for the **Aniheim Rendering Engine**, a Node.js application responsible for generating animated episodes based on AI-generated assets. The engine will support two primary modes of operation:

1.  **Web Server Mode**: Serves episodes via a web interface for viewing. 💻
2.  **File Rendering Mode**: Renders episodes to video files for offline distribution. 💾

The core architecture prioritizes **modularity** and **reproducibility**, leveraging plain-text formats for compatibility with language model generation pipelines. 📜➡️🤖

## 2. Architecture

The system comprises three main components:

*   **Input Processor**: Parses and validates input files (story, characters, etc.).
*   **Core Rendering Pipeline**: Orchestrates asset loading, synchronization, and frame generation using WebGL.
*   **Output Adapters**:
    *   `WebServer`: An Express.js application serving the rendered content.
    *   `FileRenderer`: A CLI tool using the Core Pipeline to output video files.

```mermaid
graph TD
    A[Input Files (.md, .js, .txt, .midi_txt)] --> B(Input Processor);
    B --> C{Core Rendering Pipeline};
    C --> D[WebGL Frame Generation];
    D --> E[WebServer (Express.js)];
    D --> F[FileRenderer (CLI + ffmpeg)];
    E --> G[Web Browser Output];
    F --> H[Video File Output (.mp4)];

    subgraph Inputs
        A
    end
    subgraph Processing
        B
        C
        D
    end
    subgraph Outputs
        E
        F
        G
        H
    end
```

## 3. Input Formats

Following the `CONCEPT.md`, inputs will be primarily plain text:

*   **Story/Scenes**: Markdown (`.md`) with custom directives for scene structure, character actions, dialog cues, and camera movements. Example:
    ```markdown
    # Episode 1: The Ghost in the Automaton 👻🤖

    ## Scene 1
    🪄✨ scene(setting: haunted_workshop, characters: [cogsworth, spectrina])

    **Cogsworth**: (Tinkering) Frege would be fascinated by this logic gate! ⚙️
    **Spectrina**: (Floating nearby) Boo! 👻 Did I scare your deterministic state?
    🪄✨ action(character: cogsworth, type: jump_scare)
    🪄✨ camera(zoom: character_cogsworth, duration: 2s)

    ## Scene 2
    ...
    ```
*   **Characters**: JavaScript modules (`.js`) defining character properties, associated artwork assets (references), and potentially basic behaviours or state machines. Maybe referencing Frege? 🤔 Okay, maybe not *Frege* directly in the character code, but the *spirit* of formal systems! ✨
*   **Artwork Assets**: References within Character modules, pointing to image files (e.g., PNGs for paper cutouts) generated by external models.
*   **Poses/Expressions**: A database (perhaps JSON or JS modules) mapping pose/expression names to skeletal configurations or image variations.
*   **Music**: Text-based notation (e.g., simplified MML or custom format) in `.txt` files, convertible to MIDI.
*   **Dialog**: Plain text (`.txt`) files referenced by the story Markdown.

## 4. Core Rendering Pipeline

The pipeline executes the following steps sequentially for an episode:

1.  **Parse Story**: The Input Processor reads the main story Markdown file.
2.  **Load Assets**: Dynamically loads referenced Character modules, artwork, poses, music scores, and dialog files.
3.  **Generate Audio**:
    *   Synthesizes dialog using TTS models.
    *   Renders music notation to MIDI and then to an audio format (e.g., WAV).
    *   Mixes dialog, music, and sound effects into a final scene audio track.
4.  **Setup WebGL Scene**: Initializes a WebGL context (potentially using a library like `Three.js`). Creates paper-cutout style characters with appropriate textures and simple geometry. Implements basic lighting for shadows.
5.  **Animate Frames**:
    *   Iterates through the story timeline frame by frame (or scene by scene).
    *   Updates character positions, poses (swapping textures/adjusting skeletons), and expressions based on story directives.
    *   Moves the virtual camera.
    *   Renders each frame using WebGL.
6.  **Output**: Passes rendered frames and final audio to the selected Output Adapter.

## 5. Output Adapters

### 5.1. WebServer (Node.js + Express)

*   Provides a simple web UI to browse and select episodes.
*   On request, triggers the Core Rendering Pipeline.
*   Streams the generated animation (potentially rendering on-the-fly or serving pre-rendered files) using WebGL in the browser. Audio is synchronized via HTML5 `<audio>`.
*   Uses server-side rendering or a suitable frontend framework.

### 5.2. FileRenderer (Node.js CLI)

*   Accepts input story file path and output file path as arguments.
*   Runs the Core Rendering Pipeline in a headless environment. A headless browser instance (e.g., Puppeteer) will likely be needed to run WebGL and capture frames.
*   Captures rendered frames from the headless browser.
*   Combines captured frames and the final audio track into a video file (e.g., MP4) using `ffmpeg` or a similar library/tool. 🪄✨ ffmpegMagic()! ✨🪄 (Just kidding... mostly 😜).

## 6. Technology Stack (Proposed)

*   **Runtime**: Node.js
*   **Web Framework**: Express.js
*   **WebGL**: Three.js (or similar)
*   **Headless Browser (for file rendering)**: Puppeteer
*   **Video Encoding**: `fluent-ffmpeg` (Node wrapper for ffmpeg)
*   **Audio Processing**: MIDI synthesis library (e.g., `jzz-synth-tiny`), audio mixing library.
*   **TTS**: Cloud provider APIs (e.g., Google Cloud TTS, AWS Polly) or offline models.

## 7. Future Considerations

*   **Asset Database**: A more robust system for managing generated assets.
*   **Real-time Collaboration**: Interfaces for multiple users/models to contribute.
*   **Advanced Animation**: Skeletal animation, physics simulation (because physics is fun! 👻⚛️).
*   **Caching**: Cache rendered assets/frames to speed up repeated views/renders.

This design provides a solid foundation for our phantom automaton animation studio! 🧛‍♂️🤖🎬